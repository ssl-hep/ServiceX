{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ServiceX ServiceX , a component of the IRIS-HEP Intelligent Data Delivery Service, is an experiment-agnostic service to enable on-demand columnar data delivery tailored for nearly interactive, high performance, array-based Pythonic analyses. It provides a uniform backend interface to data storage services and an intuitive frontend for users to enable columnar transformations from multiple different data formats and organizational structures.","title":"Home"},{"location":"#welcome-to-servicex","text":"ServiceX , a component of the IRIS-HEP Intelligent Data Delivery Service, is an experiment-agnostic service to enable on-demand columnar data delivery tailored for nearly interactive, high performance, array-based Pythonic analyses. It provides a uniform backend interface to data storage services and an intuitive frontend for users to enable columnar transformations from multiple different data formats and organizational structures.","title":"Welcome to ServiceX"},{"location":"about/","text":"Gortyniaco esse Parente cum essemus felix iam excussa castra Lorem markdownum imagine Cecropis aestus. Cingo avem possunt constitit alvum et genus aut plurima, nomen, non cingentibus raptaque parentum, sit aut? Non equidem amans subito, oscula contingent Sparten excutiuntque potes. Verba aut erit timorem, usum conplexibus foret nec clausit; quod? Animum tacito Dextra Acheronte meorum Penetralia consistere bonis In enim multiplicique dextra Est elisa, unum sed et custodes lacerum . Obrutaque et tectus tempus utque, venit quoque parentum tempore honores cessant qui movent me fer. Te quam aut trium et damna facit vestigia agros nostris manus. Macies in atra templis volucri nudae praecordia sua promissaque mihi. Alienae per est abstulit, dent sensit Anchisae imagine parva indicium; ad adhuc nisi populo terris signorum saecula volvitur. Hecabesque muta, formam facta; Hectoreis nobilitas terga Cephalus o resque in valet. Detinuit relicta tenaci Atque intrare ut equorum Alcathoen adspergit ingenti e tutus, labores morientibus semper Oenidae et. Acutae glandes dextrae oscula. Fert tecta animoque misit contulit, et gemuit et aetas maximus ignes. Tremulo aegro supremumque minimam arbore. Umero callem nexilibusque Hesperiosque postquam, vox per quod fieri concordes poterat dentes terrae. Festumque ad nisi Philemona saecula. Ecce stipite fugante , arma suos tendens dies Dictaei verterit in cum enim sublimis moles rorantia vertice tigres. Acta vellere cum est est cruribus motos locumque Lucifer, otia. Iubet arsuris corpora vincite ipsa deus tetigit. Virgo et loqui pugnacem cautum novitate. Simois illa, virtus belli: silvis dignum. Sed texta sinu peremi choreas et paratus nomen . Adeo cur sepulchro et ita similes perdite Quirites peioris lentum adest, crinales muris doctas adspexisse. Speciem hostibus templo patrios ; est fugam iram at caeli iam iubet es. Servantis senior ipse videre rapuere fertur relinquit ab Colcha hausta; vero Elide, via dura et. Quibus silentia neque Phoci inundet at se unde namque, unca ripis. In sibi tactuque meritique parsque preces. Ad solis : mixto acceperat conanti iustissime Eurus, et visu; te e veluti crescit, quaeque, sub.","title":"Gortyniaco esse"},{"location":"about/#gortyniaco-esse","text":"","title":"Gortyniaco esse"},{"location":"about/#parente-cum-essemus-felix-iam-excussa-castra","text":"Lorem markdownum imagine Cecropis aestus. Cingo avem possunt constitit alvum et genus aut plurima, nomen, non cingentibus raptaque parentum, sit aut? Non equidem amans subito, oscula contingent Sparten excutiuntque potes. Verba aut erit timorem, usum conplexibus foret nec clausit; quod? Animum tacito Dextra Acheronte meorum Penetralia consistere bonis In enim multiplicique dextra Est elisa, unum sed et custodes lacerum . Obrutaque et tectus tempus utque, venit quoque parentum tempore honores cessant qui movent me fer. Te quam aut trium et damna facit vestigia agros nostris manus. Macies in atra templis volucri nudae praecordia sua promissaque mihi. Alienae per est abstulit, dent sensit Anchisae imagine parva indicium; ad adhuc nisi populo terris signorum saecula volvitur. Hecabesque muta, formam facta; Hectoreis nobilitas terga Cephalus o resque in valet.","title":"Parente cum essemus felix iam excussa castra"},{"location":"about/#detinuit-relicta-tenaci","text":"Atque intrare ut equorum Alcathoen adspergit ingenti e tutus, labores morientibus semper Oenidae et. Acutae glandes dextrae oscula. Fert tecta animoque misit contulit, et gemuit et aetas maximus ignes. Tremulo aegro supremumque minimam arbore. Umero callem nexilibusque Hesperiosque postquam, vox per quod fieri concordes poterat dentes terrae. Festumque ad nisi Philemona saecula. Ecce stipite fugante , arma suos tendens dies Dictaei verterit in cum enim sublimis moles rorantia vertice tigres. Acta vellere cum est est cruribus motos locumque Lucifer, otia. Iubet arsuris corpora vincite ipsa deus tetigit. Virgo et loqui pugnacem cautum novitate. Simois illa, virtus belli: silvis dignum. Sed texta sinu peremi choreas et paratus nomen . Adeo cur sepulchro et ita similes perdite Quirites peioris lentum adest, crinales muris doctas adspexisse. Speciem hostibus templo patrios ; est fugam iram at caeli iam iubet es. Servantis senior ipse videre rapuere fertur relinquit ab Colcha hausta; vero Elide, via dura et. Quibus silentia neque Phoci inundet at se unde namque, unca ripis. In sibi tactuque meritique parsque preces. Ad solis : mixto acceperat conanti iustissime Eurus, et visu; te e veluti crescit, quaeque, sub.","title":"Detinuit relicta tenaci"},{"location":"development/","text":"Development So you want to get involved! The developers welcome community input. The central development area with all ServiceX repositories can be found here . The core steps are shown in the diagram below. Testing new changes [Instructions for testing new changes. Broadly (1) setup testing environment via conda or virtualenv , (2) clone the appropriate repository, (3) run python -m pip install -e .[test] to set up the necessary packages in the environment, and (4) run the tests via pytests .] Deploying ServiceX on a Kubernetes cluster The entire ServiceX stack can be installed using the helm chart contained here . Below is a set of developer instructions for deploying a production-ready instance of ServiceX on a Kubernetes cluster. Deployment requires access to a Kubernetes cluster. You\u2019ll need to install and set up kubectl (see Install and Set Up kubectl ) and Helm 3 (see Installing Helm ). Next configure kubectl to access the appropriate namespace on the cluster with the kubeconfig file located at ~/.kube/config . For complete control over the deployment values in the ServiceX Helm chart, it\u2019s recommended to check out the develop branch of ServiceX from GitHub: git clone https://github.com/ssl-hep/ServiceX.git cd ServiceX git checkout develop Add and update the ServiceX Helm chart: helm repo add ssl-hep https://ssl-hep.github.io/ssl-helm-charts/ helm repo update helm dependency update servicex ServiceX may require some modifications to the default deployment values to run on your cluster. In particular, you\u2019ll need your CERN grid certificate to communicate with Rucio. You can see the default values in servicex/values.yaml . These values can be overridden via a separate yaml file. For example, to run on SSL-RIVER, cat <<EOF > river-values.yaml app: auth: true adminUser: admin adminPassword: eto2ipiis1 ingress: enabled: true didFinder: tag: v1.0-rc.1 pullPolicy: Always codeGen: # image: sslhep/servicex_code_gen_func_adl_uproot image: sslhep/servicex_code_gen_func_adl_xaod rabbitmq: service: type: NodePort nodePort: 30672 minio: ingress: enabled: true hosts: - \"servicex-minio.uc.ssl-hep.org\" gridAccount: bgalewsk EOF With the appropriate values, it is now possible to deploy ServiceX via the Helm chart: helm install -f river-values.yaml --version v1.0.0-rc.1 rc1-xaod ssl-hep/servicex Initial deployment is typically rapid, with RabbitMQ requiring up to a minute to complete its initialization. After this all the pods of the new deployment should be ready. If you check the status of the pods via kubectl get pods You should soon find that your setup looks comparable to this: NAME READY STATUS RESTARTS AGE servicex-1579021789-code-gen-7cd998d5b6-nwtgv 1/1 Running 0 49m servicex-1579021789-did-finder-7c5cbb4575-52wxf 1/1 Running 0 49m servicex-1579021789-minio-78b55bfdf8-mbmmf 1/1 Running 0 49m servicex-1579021789-preflight-b748b4dfd-qqt89 1/1 Running 4 49m servicex-1579021789-rabbitmq-0 1/1 Running 0 49m servicex-1579021789-servicex-app-98779c79c-cvmqx 1/1 Running 3 49m servicex-1579021789-x509-secrets-74f4bcc8bb-5kqvb 1/1 Running 0 49m The new ServiceX deployment should now be ready for business. To try a 10TB scale test check ServiceX_scaleTest .","title":"Development"},{"location":"development/#development","text":"So you want to get involved! The developers welcome community input. The central development area with all ServiceX repositories can be found here . The core steps are shown in the diagram below.","title":"Development"},{"location":"development/#testing-new-changes","text":"[Instructions for testing new changes. Broadly (1) setup testing environment via conda or virtualenv , (2) clone the appropriate repository, (3) run python -m pip install -e .[test] to set up the necessary packages in the environment, and (4) run the tests via pytests .]","title":"Testing new changes"},{"location":"development/#deploying-servicex-on-a-kubernetes-cluster","text":"The entire ServiceX stack can be installed using the helm chart contained here . Below is a set of developer instructions for deploying a production-ready instance of ServiceX on a Kubernetes cluster. Deployment requires access to a Kubernetes cluster. You\u2019ll need to install and set up kubectl (see Install and Set Up kubectl ) and Helm 3 (see Installing Helm ). Next configure kubectl to access the appropriate namespace on the cluster with the kubeconfig file located at ~/.kube/config . For complete control over the deployment values in the ServiceX Helm chart, it\u2019s recommended to check out the develop branch of ServiceX from GitHub: git clone https://github.com/ssl-hep/ServiceX.git cd ServiceX git checkout develop Add and update the ServiceX Helm chart: helm repo add ssl-hep https://ssl-hep.github.io/ssl-helm-charts/ helm repo update helm dependency update servicex ServiceX may require some modifications to the default deployment values to run on your cluster. In particular, you\u2019ll need your CERN grid certificate to communicate with Rucio. You can see the default values in servicex/values.yaml . These values can be overridden via a separate yaml file. For example, to run on SSL-RIVER, cat <<EOF > river-values.yaml app: auth: true adminUser: admin adminPassword: eto2ipiis1 ingress: enabled: true didFinder: tag: v1.0-rc.1 pullPolicy: Always codeGen: # image: sslhep/servicex_code_gen_func_adl_uproot image: sslhep/servicex_code_gen_func_adl_xaod rabbitmq: service: type: NodePort nodePort: 30672 minio: ingress: enabled: true hosts: - \"servicex-minio.uc.ssl-hep.org\" gridAccount: bgalewsk EOF With the appropriate values, it is now possible to deploy ServiceX via the Helm chart: helm install -f river-values.yaml --version v1.0.0-rc.1 rc1-xaod ssl-hep/servicex Initial deployment is typically rapid, with RabbitMQ requiring up to a minute to complete its initialization. After this all the pods of the new deployment should be ready. If you check the status of the pods via kubectl get pods You should soon find that your setup looks comparable to this: NAME READY STATUS RESTARTS AGE servicex-1579021789-code-gen-7cd998d5b6-nwtgv 1/1 Running 0 49m servicex-1579021789-did-finder-7c5cbb4575-52wxf 1/1 Running 0 49m servicex-1579021789-minio-78b55bfdf8-mbmmf 1/1 Running 0 49m servicex-1579021789-preflight-b748b4dfd-qqt89 1/1 Running 4 49m servicex-1579021789-rabbitmq-0 1/1 Running 0 49m servicex-1579021789-servicex-app-98779c79c-cvmqx 1/1 Running 3 49m servicex-1579021789-x509-secrets-74f4bcc8bb-5kqvb 1/1 Running 0 49m The new ServiceX deployment should now be ready for business. To try a 10TB scale test check ServiceX_scaleTest .","title":"Deploying ServiceX on a Kubernetes cluster"},{"location":"examples/","text":"Examples Some intro to the various ways in which the service can be used. Will add more here, but for now the best info is probably in the notebooks below. Transform xAOD files from Rucio How to transform xAOD files from Rucio Extract columns from flat ntuples in Rucio How to extract columns from flat ntuples Transform a list of miniAOD files Some cutting-edge stuff for CMS formats","title":"Examples"},{"location":"examples/#examples","text":"Some intro to the various ways in which the service can be used. Will add more here, but for now the best info is probably in the notebooks below.","title":"Examples"},{"location":"examples/#transform-xaod-files-from-rucio","text":"How to transform xAOD files from Rucio","title":"Transform xAOD files from Rucio"},{"location":"examples/#extract-columns-from-flat-ntuples-in-rucio","text":"How to extract columns from flat ntuples","title":"Extract columns from flat ntuples in Rucio"},{"location":"examples/#transform-a-list-of-miniaod-files","text":"Some cutting-edge stuff for CMS formats","title":"Transform a list of miniAOD files"},{"location":"installation/","text":"Getting started for users This part is still a little sparse, as details are changing rapidly. Interacting with a central instance of ServiceX (as opposed to setting up your own instance) consists in two parts: getting authenticated in the system by an administrator and installing the appropriate frontend library. Getting authenticated There are two instances of ServiceX, one to transform xAOD input files and one to transform flat ntuples, and you must be separately authenticated to each in order to use them. You can go to the xAOD registration or the Uproot registration and put in the username and password for your requested account. In addition, both instances rely on ATLAS credentials to access Rucio, so you must be a member of the ATLAS VO to use them. In these early days the ServiceX admins will seek to personally accept pending accounts, so once you've registered send a message to the admins (Ben Galewsky or Marc Weinberg, e.g. via Slack) and they will authorize any pending requests. Once you\u2019re authenticated by an administrator your account will have access to ServiceX, and you\u2019ll be able to put in transform requests. Installing the frontend Python library The documentation for the ServiceX frontend is shown here . It's also useful to employ functions from the func-adl libraries ( for xAOD or Uproot ). To interact with ServiceX via the frontend library you\u2019ll need an environment running Python 3.7 or later: python -m pip install servicex==2.0.0b6 python -m pip install func-adl-xAOD==1.1.0b4 After the prompt you can import the libraries and make a request using your registered username and password.","title":"Getting started"},{"location":"installation/#getting-started-for-users","text":"This part is still a little sparse, as details are changing rapidly. Interacting with a central instance of ServiceX (as opposed to setting up your own instance) consists in two parts: getting authenticated in the system by an administrator and installing the appropriate frontend library.","title":"Getting started for users"},{"location":"installation/#getting-authenticated","text":"There are two instances of ServiceX, one to transform xAOD input files and one to transform flat ntuples, and you must be separately authenticated to each in order to use them. You can go to the xAOD registration or the Uproot registration and put in the username and password for your requested account. In addition, both instances rely on ATLAS credentials to access Rucio, so you must be a member of the ATLAS VO to use them. In these early days the ServiceX admins will seek to personally accept pending accounts, so once you've registered send a message to the admins (Ben Galewsky or Marc Weinberg, e.g. via Slack) and they will authorize any pending requests. Once you\u2019re authenticated by an administrator your account will have access to ServiceX, and you\u2019ll be able to put in transform requests.","title":"Getting authenticated"},{"location":"installation/#installing-the-frontend-python-library","text":"The documentation for the ServiceX frontend is shown here . It's also useful to employ functions from the func-adl libraries ( for xAOD or Uproot ). To interact with ServiceX via the frontend library you\u2019ll need an environment running Python 3.7 or later: python -m pip install servicex==2.0.0b6 python -m pip install func-adl-xAOD==1.1.0b4 After the prompt you can import the libraries and make a request using your registered username and password.","title":"Installing the frontend Python library"},{"location":"introduction/","text":"Introduction The High Luminosity Large Hadron Collider (HL-LHC) faces enormous computational challenges in the 2020s. The HL-LHC will produce exabytes of data each year, with increasingly complex event structure due to high pileup conditions. The ATLAS and CMS experiments will record ~ 10 times as much data from ~ 100 times as many collisions as were used to discover the Higgs boson. Columnar data delivery ServiceX seeks to enable on-demand data delivery of columnar data in a variety of formats for physics analyses. It provides a uniform backend to data storage services, ensuring the user doesn't have to know how or where the data is stored, and is capable of on-the-fly data transformations into a variety of formats (ROOT files, Arrow arrays, Parquet files, ...) The service offers preprocessing functionality via an analysis description language called func-adl that allows users to filter events in place, unpack compressed formats, request columns, and specify computations to be applied to the results. This enables the user to start from any format and extract only the data needed for an analysis. As shown in the diagram, ServiceX is designed to feed columns to a user running an analysis (e.g. via awkward or coffea tools) based on the results of a query designed by the user.","title":"Introduction"},{"location":"introduction/#introduction","text":"The High Luminosity Large Hadron Collider (HL-LHC) faces enormous computational challenges in the 2020s. The HL-LHC will produce exabytes of data each year, with increasingly complex event structure due to high pileup conditions. The ATLAS and CMS experiments will record ~ 10 times as much data from ~ 100 times as many collisions as were used to discover the Higgs boson.","title":"Introduction"},{"location":"introduction/#columnar-data-delivery","text":"ServiceX seeks to enable on-demand data delivery of columnar data in a variety of formats for physics analyses. It provides a uniform backend to data storage services, ensuring the user doesn't have to know how or where the data is stored, and is capable of on-the-fly data transformations into a variety of formats (ROOT files, Arrow arrays, Parquet files, ...) The service offers preprocessing functionality via an analysis description language called func-adl that allows users to filter events in place, unpack compressed formats, request columns, and specify computations to be applied to the results. This enables the user to start from any format and extract only the data needed for an analysis. As shown in the diagram, ServiceX is designed to feed columns to a user running an analysis (e.g. via awkward or coffea tools) based on the results of a query designed by the user.","title":"Columnar data delivery"},{"location":"outreach/","text":"Outreach Information available in the HEP community. Presentations List of talks/posters to the HEP community on ServiceX IRIS-HEP Poster Session 2020 at Princeton University ServiceX talk at CHEP 2019 ServiceX talk at HSF DAWG \u2013 DOMA Access meeting Tutorials A link to our tutorials. We currently have two: A streaming example for xAOD files using func_adl An example using hep_tables for the frontend","title":"Outreach"},{"location":"outreach/#outreach","text":"Information available in the HEP community.","title":"Outreach"},{"location":"outreach/#presentations","text":"List of talks/posters to the HEP community on ServiceX IRIS-HEP Poster Session 2020 at Princeton University ServiceX talk at CHEP 2019 ServiceX talk at HSF DAWG \u2013 DOMA Access meeting","title":"Presentations"},{"location":"outreach/#tutorials","text":"A link to our tutorials. We currently have two: A streaming example for xAOD files using func_adl An example using hep_tables for the frontend","title":"Tutorials"},{"location":"requests/","text":"Specifying a request A transformation request is a specifically formatted request sent to ServiceX. It includes information on what input dataset is to be used, what preselection is to be applied (including computation of new columns, if any), and what columns should be returned to the user. Creating a request via func_adl In order to use func_adl directly we start with a Qastle query as our input. We start from a query designed to extract the four-momenta of the Electron collection in some dataset: my_query = \"(Select data_column_source\" \\ \"(lambda (list Event)\" \\ \"(list (call (attr (attr Event 'Electrons') 'pt'))\" \\ \"(call (attr (attr Event 'Electrons') 'eta'))\" \\ \"(call (attr (attr Event 'Electrons') 'phi'))\" \\ \"(call (attr (attr Event 'Electrons') 'e')))))\" Given this input, we can produce a pandas.DataFrame containing the four momenta of all electrons in an ATLAS xAOD file via import servicex dataset = \u2018mc15_13TeV:mc15_13TeV.361106.PowhegPythia8EvtGen_AZNLOCTEQ6L1_Zee.merge.DAOD_STDM3.e3601_s2576_s2132_r6630_r6264_p2363_tid05630052_00\u2019 sx_endpoint = 'http://rc1-xaod-servicex.uc.ssl-hep.org' minio_endpoint = 'servicex-minio.uc.ssl-hep.org' ds = servicex.ServiceXDataset( dataset, servicex.ServiceXAdaptor(sx_endpoint, username='mweinberg', password='XXXXXXXXX'), servicex.MinioAdaptor(minio_endpoint) ) r = servicex.get_data_pandas_df(my_query) print(r) This queries ServiceX for the specified dataset, extracts the requested columns, and after about 1--2 minutes, prints a data frame with 4 columns (one for each of the variables). A badly formatted query, or a problem with the file in the backend, will cause an exception to be thrown. Note that there are also tools like the one here that are capable of turning a text file of requested columns (e.g. here ) into a complete Qastle query. For all but the simplest single-column requests, creating a Qastle query as input can be quite cumbersome. func_adl provides additional libraries to construct queries. For example import func_adl_xAOD f_ds = func_adl_xAOD.ServiceXDatasetSource(ds) r = f_ds \\ .SelectMany('lambda e: e.Jets(\"AntiKt4EMTopoJets\")') \\ .Select('lambda j: j.pt()/1000.0') \\ .AsPandasDF('JetPt') \\ .value() print(r) Creating a request via hep_tables Another tool that can be used to make these requests is hep_tables . This can (soon) be installed via pip: pip install hep_tables As a simple example of their use, we can make a plot of the electron transverse momenta. First we set up the dataset and a computational graph for func_adl : from hep_tables import xaod_table, make_local from func_adl import EventDataset import matplotlib.pyplot as plt dataset = EventDataset(\u2018localds://mc15_13TeV:mc15_13TeV.361106.PowhegPythia8EvtGen_AZNLOCTEQ6L1_Zee.merge.DAOD_STDM3.e3601_s2576_s2132_r6630_r6264_p2363_tid05630052_00') df = xaod_table(dataset) Once we have this we can create a query for the electron transverse momenta and run it via ServiceX: pts = df.Electrons(\u2018Electrons\u2019).pt / 1000.0 np_pts = make_local(pts) This translates the query into the appropriate format for ServiceX, sends it to the service, collects their results, and packages them locally as a JaggedArray. Finally we can make a plot from this via: plt.hist(np_pts.flatten(), range=(0,100), bins=50) Which results in a histogram of a transverse momentum distribution: Small example with filtering Will include a small toy example with filtering here.","title":"Specifying a request"},{"location":"requests/#specifying-a-request","text":"A transformation request is a specifically formatted request sent to ServiceX. It includes information on what input dataset is to be used, what preselection is to be applied (including computation of new columns, if any), and what columns should be returned to the user.","title":"Specifying a request"},{"location":"requests/#creating-a-request-via-func_adl","text":"In order to use func_adl directly we start with a Qastle query as our input. We start from a query designed to extract the four-momenta of the Electron collection in some dataset: my_query = \"(Select data_column_source\" \\ \"(lambda (list Event)\" \\ \"(list (call (attr (attr Event 'Electrons') 'pt'))\" \\ \"(call (attr (attr Event 'Electrons') 'eta'))\" \\ \"(call (attr (attr Event 'Electrons') 'phi'))\" \\ \"(call (attr (attr Event 'Electrons') 'e')))))\" Given this input, we can produce a pandas.DataFrame containing the four momenta of all electrons in an ATLAS xAOD file via import servicex dataset = \u2018mc15_13TeV:mc15_13TeV.361106.PowhegPythia8EvtGen_AZNLOCTEQ6L1_Zee.merge.DAOD_STDM3.e3601_s2576_s2132_r6630_r6264_p2363_tid05630052_00\u2019 sx_endpoint = 'http://rc1-xaod-servicex.uc.ssl-hep.org' minio_endpoint = 'servicex-minio.uc.ssl-hep.org' ds = servicex.ServiceXDataset( dataset, servicex.ServiceXAdaptor(sx_endpoint, username='mweinberg', password='XXXXXXXXX'), servicex.MinioAdaptor(minio_endpoint) ) r = servicex.get_data_pandas_df(my_query) print(r) This queries ServiceX for the specified dataset, extracts the requested columns, and after about 1--2 minutes, prints a data frame with 4 columns (one for each of the variables). A badly formatted query, or a problem with the file in the backend, will cause an exception to be thrown. Note that there are also tools like the one here that are capable of turning a text file of requested columns (e.g. here ) into a complete Qastle query. For all but the simplest single-column requests, creating a Qastle query as input can be quite cumbersome. func_adl provides additional libraries to construct queries. For example import func_adl_xAOD f_ds = func_adl_xAOD.ServiceXDatasetSource(ds) r = f_ds \\ .SelectMany('lambda e: e.Jets(\"AntiKt4EMTopoJets\")') \\ .Select('lambda j: j.pt()/1000.0') \\ .AsPandasDF('JetPt') \\ .value() print(r)","title":"Creating a request via func_adl"},{"location":"requests/#creating-a-request-via-hep_tables","text":"Another tool that can be used to make these requests is hep_tables . This can (soon) be installed via pip: pip install hep_tables As a simple example of their use, we can make a plot of the electron transverse momenta. First we set up the dataset and a computational graph for func_adl : from hep_tables import xaod_table, make_local from func_adl import EventDataset import matplotlib.pyplot as plt dataset = EventDataset(\u2018localds://mc15_13TeV:mc15_13TeV.361106.PowhegPythia8EvtGen_AZNLOCTEQ6L1_Zee.merge.DAOD_STDM3.e3601_s2576_s2132_r6630_r6264_p2363_tid05630052_00') df = xaod_table(dataset) Once we have this we can create a query for the electron transverse momenta and run it via ServiceX: pts = df.Electrons(\u2018Electrons\u2019).pt / 1000.0 np_pts = make_local(pts) This translates the query into the appropriate format for ServiceX, sends it to the service, collects their results, and packages them locally as a JaggedArray. Finally we can make a plot from this via: plt.hist(np_pts.flatten(), range=(0,100), bins=50) Which results in a histogram of a transverse momentum distribution:","title":"Creating a request via hep_tables"},{"location":"requests/#small-example-with-filtering","text":"Will include a small toy example with filtering here.","title":"Small example with filtering"}]}